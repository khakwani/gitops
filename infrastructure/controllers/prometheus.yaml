# Namespace creation for monitoring
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    app.kubernetes.io/component: monitoring
---
# Helm repository for Prometheus
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: prometheus-community
  namespace: monitoring
spec:
  interval: 12h
  type: oci
  url: oci://ghcr.io/prometheus-community/charts
---
# Helm repository for Grafana Loki
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: grafana
  namespace: monitoring
spec:
  interval: 12h
  url: https://grafana.github.io/helm-charts
---
# Helm release for kube-prometheus-stack
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 1h
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "58.7.2"
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
      interval: 1h
  values:
    alertmanager:
      enabled: true
      alertmanagerSpec:
        config:
          global:
            resolve_timeout: 5m
          route:
            group_by: ['alertname', 'job']
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 5m
            receiver: 'null'
            routes:
              - match:
                  alertname: Watchdog
                receiver: 'null'
          receivers:
            - name: 'null'
    prometheus:
      prometheusSpec:
        storageSpec:
          volumeClaimTemplate:
            spec:
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 10Gi
        alerting:
          alertmanagers:
            - static_configs:
                - targets:
                    - 'kube-prometheus-stack-alertmanager.monitoring:9093'
      nodeSelector:
        kubernetes.io/hostname: staging-worker
      tolerations:
        - key: "node-role.kubernetes.io/worker"
          operator: "Exists"
          effect: "NoSchedule"
      retention: 24h
      resources:
        limits:
          cpu: 2
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 500Mi
      podMonitorNamespaceSelector:
        any: true
      podMonitorSelector: {}
      podMonitorSelectorNilUsesHelmValues: false
      ruleNamespaceSelector:
        any: true
      ruleSelector: {}
      ruleSelectorNilUsesHelmValues: false
      serviceMonitorNamespaceSelector:
        matchNames:
          - monitoring
      serviceMonitorSelector: {}
      serviceMonitorSelectorNilUsesHelmValues: false
      probeNamespaceSelector:
        any: true
      probeSelector: {}
      probeSelectorNilUsesHelmValues: false
---
# Helm release for Grafana Loki with Promtail configuration
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: grafana-loki
  namespace: monitoring
spec:
  interval: 1h
  chart:
    spec:
      chart: loki-stack
      version: "2.5.0" # Updated version to avoid version mismatch
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: monitoring
  values:
    loki:
      enabled: true
      persistence:
        enabled: true
        storageClassName: "standard"
        accessModes:
          - ReadWriteOnce
        size: 10Gi
    promtail:
      enabled: true
      config:
        clients:
          - url: http://loki:3100/loki/api/v1/push
            tenant_id: 1
        positions:
          filename: /var/log/positions.yaml
        scrape_configs:
          - job_name: system
            static_configs:
              - targets:
                  - localhost
                labels:
                  job: varlogs
                  host: ${HOSTNAME}
                  __path__: /var/log/*log
    rbac:
      create: true
      pspEnabled: false
    serviceMonitorNamespaceSelector:
      matchNames:
        - monitoring
    serviceMonitorSelector: {}
    serviceMonitorSelectorNilUsesHelmValues: false
    probeNamespaceSelector:
      any: true
    probeSelector: {}
    probeSelectorNilUsesHelmValues: false
    prometheus:
      additionalScrapeConfigs:
        - job_name: 'promtail'
          static_configs:
            - targets: ['promtail.monitoring.svc:3101']
---
# ServiceAccount for Grafana Loki
apiVersion: v1
kind: ServiceAccount
metadata:
  name: loki
  namespace: monitoring
---
# Role for Grafana Loki
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: loki
  namespace: monitoring
rules:
  - apiGroups: [""]
    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["ingresses"]
    verbs: ["list", "watch"]
  - apiGroups: ["networking.k8s.io"]
    resources: ["networkpolicies"]
    verbs: ["list", "watch"]
---
# RoleBinding for Grafana Loki
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: loki
  namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: loki
subjects:
  - kind: ServiceAccount
    name: loki
    namespace: monitoring
---
# ConfigMap for Grafana Loki
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: monitoring
data:
  loki-local-config.yaml: |
    auth_enabled: false
    server:
      http_listen_port: 3100
    ingester:
      lifecycler:
        address: 127.0.0.1
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
        final_sleep: 0s
    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/index
        cache_location: /loki/cache
        shared_store: filesystem
      filesystem:
        directory: /loki/chunks
    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h
    chunk_store_config:
      max_look_back_period: 0s
    table_manager:
      retention_deletes_enabled: true
      retention_period: 336h
---
# StatefulSet for Grafana Loki
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: loki
  namespace: monitoring
spec:
  serviceName: loki
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      serviceAccountName: loki
      containers:
        - name: loki
          image: grafana/loki:2.6.1
          ports:
            - containerPort: 3100
          readinessProbe:
            httpGet:
              path: /ready
              port: 3100
            initialDelaySeconds: 45
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1By adding the Promtail configuration directly into the `prometheus.yaml` file and using the HelmRelease approach, you're streamlining the deployment of all monitoring-related tools (Prometheus, Loki, and Promtail) through a single Helm chart. This keeps things simple and ensures that all changes and configurations are version-controlled and managed as part of the GitOps process.

### Key Steps:
1. **Promtail DaemonSet** is included in the HelmRelease for Loki within `prometheus.yaml`.
2. **Promtail Configuration** is embedded directly in the same file under the `values:` section for `grafana-loki`. This sends logs to the Loki instance, so you donâ€™t need a separate `values.yaml` file anymore.

### Benefits of This Approach:
- **Centralized Configuration**: Promtail's setup is now embedded into your GitOps workflow and stored with the rest of your cluster configurations.
- **Automation**: Flux will automatically reconcile and apply changes, ensuring that Promtail is installed or updated alongside Loki and Prometheus.
- **Consistency**: All configurations (Prometheus, Loki, Promtail) will be in sync, avoiding misconfigurations across different files.

Once this is deployed, you should be able to check your cluster for Promtail DaemonSets and see logs being collected by Promtail and pushed to Loki.
