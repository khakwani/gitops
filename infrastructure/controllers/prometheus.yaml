apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    app.kubernetes.io/component: monitoring
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: prometheus-community
  namespace: monitoring
spec:
  interval: 12h
  type: oci
  url: oci://ghcr.io/prometheus-community/charts
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: loki
  namespace: monitoring
spec:
  interval: 12h
  url: https://grafana.github.io/helm-charts
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 1h
  chart:
    spec:
      version: "58.7.2"
      chart: kube-prometheus-stack
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
      interval: 1h
  install:
    crds: Create
  upgrade:
    crds: CreateReplace
  values:
    alertmanager:
      enabled: true
      alertmanagerSpec:
        config:
          global:
            resolve_timeout: 5m
          route:
            receiver: 'discord'
        receivers:
          - name: 'discord'
            webhook_configs:
              - url: "https://discordapp.com/api/webhooks/1246044427005202432/X5TYJG3z5e5BgZqfe8OxLUMpBr-_s4mny9--LBjovES9CJhFFPXb0bEOS16cLHKYXu_y"
    prometheus:
      prometheusSpec:
        alerting:
          alertmanagers:
            - static_configs:
                - targets:
                    - 'kube-prometheus-stack-alertmanager.monitoring:9093'  # Ensure this matches your Alertmanager service
        nodeSelector:
          tasktype: compute
        tolerations:
          - key: "node-role.kubernetes.io/control-plane"
            operator: "Exists"
            effect: "NoSchedule"
        retention: 24h
        resources:
          limits:
            cpu: 1
            memory: 500Mi
          requests:
            cpu: 200m
            memory: 200Mi
        podMonitorNamespaceSelector:
          any: true
        podMonitorSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        ruleNamespaceSelector:
          any: true
        ruleSelector: {}
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorNamespaceSelector:
          matchNames:
            - monitoring  # Specify namespaces to monitor here
        serviceMonitorSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        probeNamespaceSelector:
          any: true
        probeSelector: {}
        probeSelectorNilUsesHelmValues: false
      additionalAlertRelabelConfigs:
        - sourceLabels: ['__name__']
          regex: 'kube_pod_container_status_restarts_total'
          action: 'keep'
          modulatortype: 'set'
          modulus: 1
          separator: ';'
          replacement: '${1}_restarts;${2};${3}'
          targetLabel: 'pod_name'
          keep: false
          labelDrop: true
      additionalAlerts:
        - alert: PodRestartRate
          expr: rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[5m]) > 0.5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.pod }} restarted frequently"
            description: "Pod {{ $labels.pod }} restarted more than 0.5 times per minute over the last 5 minutes."
        - alert: PodCrashLoop
          expr: increase(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[5m]) > 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod {{ $labels.pod }} in crash loop"
            description: "Pod {{ $labels.pod }} restarted more than once in the last 5 minutes, indicating a crash loop."
        - alert: PodNotStarting
          expr: kube_pod_status_phase{job="kube-state-metrics"} == 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.pod }} not starting"
            description: "Pod {{ $labels.pod }} has been in a pending state for more than 10 minutes."
        - alert: PodTerminatingStuck
          expr: kube_pod_status_phase{job="kube-state-metrics"} == 1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.pod }} stuck in Terminating state"
            description: "Pod {{ $labels.pod }} has been in Terminating state for more than 10 minutes."
    grafana:
      defaultDashboardsEnabled: true
      adminPassword: "admin123"
      ingress:
        enabled: true
        ingressClassName: nginx
        annotations:
          kubernetes.io/ingress.class: nginx
          cert-manager.io/cluster-issuer: letsencrypt
          nginx.ingress.kubernetes.io/rewrite-target: /
          nginx.ingress.kubernetes.io/ssl-redirect: "false"
        hosts:
          - localhost
        tls:
          - hosts:
              - localhost
            secretName: cert-secret-mon
        path: /
      env:
        HTTP_PROXY: ""
        http_proxy: ""
        HTTPS_PROXY: ""
        https_proxy: ""
        NO_PROXY: ""
        no_proxy: ""
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: loki-stack
  namespace: monitoring
spec:
  interval: 1h
  chart:
    spec:
      chart: loki-stack
      version: "2.4.1"
      sourceRef:
        kind: HelmRepository
        name: loki
        namespace: monitoring
      interval: 1h
  values:
    loki:
      enabled: true
    promtail:
      enabled: true
    rbac:
      pspEnabled: false  # Disable PodSecurityPolicy if it exists
    serviceMonitorNamespaceSelector:
      matchNames:
        - monitoring  # Specify namespaces to monitor here
    serviceMonitorSelector: {}
    serviceMonitorSelectorNilUsesHelmValues: false
    probeNamespaceSelector:
      any: true
    probeSelector: {}
    probeSelectorNilUsesHelmValues: false
    prometheus:
      additionalScrapeConfigs:
        - job_name: 'promtail'
          static_configs:
            - targets: ['promtail:9080']  # Replace 'promtail' with the actual hostname of your Promtail service
